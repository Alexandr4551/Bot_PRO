"""
–§–ò–ù–ê–õ–¨–ù–ê–Ø –†–ê–ë–û–ß–ê–Ø –í–ï–†–°–ò–Ø - –±–µ–∑ –ø—Ä–æ–±–ª–µ–º —Å –∞–Ω—Å–∞–º–±–ª–µ–º
–û–±—É—á–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∑–∞ 3-5 –º–∏–Ω—É—Ç
"""
import pandas as pd
import numpy as np
import glob
import lightgbm as lgb
import xgboost as xgb
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.preprocessing import RobustScaler
import joblib
import logging
import os
import warnings
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from ta.momentum import RSIIndicator, StochasticOscillator, WilliamsRIndicator
from ta.trend import ADXIndicator, CCIIndicator
from ta.volatility import AverageTrueRange
from ta.volume import AccDistIndexIndicator, ChaikinMoneyFlowIndicator
from datetime import datetime
import json
from tqdm import tqdm

warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('FinalModelTrainer')

# –§–ò–ù–ê–õ–¨–ù–ê–Ø –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
CONFIG = {
    'MODEL_DIR': 'trained_models',
    'DATA_DIR': 'historical_data',
    'REPORTS_DIR': 'model_reports',
    'TEST_SIZE': 0.15,
    'VAL_SIZE': 0.15,
    'RANDOM_STATE': 42,
    'USE_SAMPLE': True,          
    'SAMPLE_SIZE': 80000,        
    'TOP_FEATURES': 35
}

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
for dir_name in [CONFIG['MODEL_DIR'], CONFIG['REPORTS_DIR']]:
    os.makedirs(dir_name, exist_ok=True)

class FinalFeatureEngineering:
    """–§–∏–Ω–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    
    def add_essential_indicators(self, df):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        try:
            logger.info("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤...")
            
            # Momentum –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            df['rsi'] = RSIIndicator(df['close'], window=14).rsi()
            df['stoch_k'] = StochasticOscillator(df['high'], df['low'], df['close']).stoch()
            df['williams_r'] = WilliamsRIndicator(df['high'], df['low'], df['close']).williams_r()
            
            # Trend –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            df['adx'] = ADXIndicator(df['high'], df['low'], df['close']).adx()
            df['cci'] = CCIIndicator(df['high'], df['low'], df['close']).cci()
            
            # Volatility
            df['atr'] = AverageTrueRange(df['high'], df['low'], df['close']).average_true_range()
            
            # Volume
            df['adi'] = AccDistIndexIndicator(df['high'], df['low'], df['close'], df['volume']).acc_dist_index()
            df['cmf'] = ChaikinMoneyFlowIndicator(df['high'], df['low'], df['close'], df['volume']).chaikin_money_flow()
            
            return df
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤: {str(e)}")
            return df
    
    def add_statistical_features(self, df):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        try:
            logger.info("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
            
            for window in [5, 20, 50]:
                df[f'close_mean_{window}'] = df['close'].rolling(window).mean()
                df[f'close_std_{window}'] = df['close'].rolling(window).std()
                df[f'close_rank_{window}'] = df['close'].rolling(window).rank(pct=True)
                df[f'close_position_{window}'] = (df['close'] - df['close'].rolling(window).min()) / (
                    df['close'].rolling(window).max() - df['close'].rolling(window).min()
                )
            
            return df
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {str(e)}")
            return df
    
    def add_price_action_features(self, df):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Ü–µ–Ω–æ–≤–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è"""
        try:
            logger.info("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Ü–µ–Ω–æ–≤–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è...")
            
            for lag in [1, 3, 5, 10]:
                df[f'momentum_{lag}'] = df['close'].pct_change(lag)
                df[f'volume_momentum_{lag}'] = df['volume'].pct_change(lag)
            
            df['body_size'] = abs(df['close'] - df['open']) / df['close']
            df['upper_shadow'] = (df['high'] - np.maximum(df['close'], df['open'])) / df['close']
            df['lower_shadow'] = (np.minimum(df['close'], df['open']) - df['low']) / df['close']
            df['hl_spread'] = (df['high'] - df['low']) / df['close']
            df['volume_price_trend'] = df['volume'] * df['momentum_1']
            
            return df
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Ü–µ–Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {str(e)}")
            return df
    
    def process_dataframe(self, df):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ DataFrame"""
        logger.info("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        if 'timestamp' in df.columns:
            df = df.set_index('timestamp')
            df.index = pd.to_datetime(df.index)
        
        df = self.add_essential_indicators(df)
        df = self.add_statistical_features(df)
        df = self.add_price_action_features(df)
        
        df = df.reset_index()
        logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(df.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        return df

class FinalModelTrainer:
    """–§–∏–Ω–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self):
        self.models = {}
        self.scaler = None
        self.feature_engineering = FinalFeatureEngineering()
        
    def load_and_prepare_data(self, symbols=None, timeframes=None):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        file_pattern = f"{CONFIG['DATA_DIR']}/*.parquet"
        files = glob.glob(file_pattern)
        
        if not files:
            raise FileNotFoundError(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –≤ {CONFIG['DATA_DIR']}")
        
        dfs = []
        for file in tqdm(files, desc="–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤"):
            if symbols or timeframes:
                filename = os.path.basename(file)
                parts = filename.split('_')
                symbol = parts[0]
                timeframe = parts[1].replace('min', '')
                
                if symbols and symbol not in symbols:
                    continue
                if timeframes and timeframe not in [str(tf) for tf in timeframes]:
                    continue
            
            df = pd.read_parquet(file)
            df = self.feature_engineering.process_dataframe(df)
            dfs.append(df)
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω {file} ({len(df)} —Å—Ç—Ä–æ–∫)")
        
        full_df = pd.concat(dfs, ignore_index=True)
        
        if CONFIG['USE_SAMPLE'] and len(full_df) > CONFIG['SAMPLE_SIZE']:
            logger.info(f"–°–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Å {len(full_df)} –¥–æ {CONFIG['SAMPLE_SIZE']} —Å—Ç—Ä–æ–∫...")
            full_df = full_df.sample(n=CONFIG['SAMPLE_SIZE'], random_state=CONFIG['RANDOM_STATE'])
        
        logger.info(f"–§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(full_df)} —Å—Ç—Ä–æ–∫")
        return self.prepare_features(full_df)
    
    def prepare_features(self, df):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        logger.info("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        df = self.create_target(df)
        
        drop_cols = ['timestamp', 'symbol', 'timeframe', 'turnover', 'target_class']
        df = df.drop(columns=[col for col in drop_cols if col in df.columns])
        
        df = df.replace([np.inf, -np.inf], np.nan)
        df = df.dropna()
        
        X = df.drop(columns=['target_final'])
        y = df['target_final']
        
        class_ratio = y.value_counts(normalize=True)
        logger.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {class_ratio.to_dict()}")
        
        return X, y
    
    def create_target(self, df):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π"""
        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π...")
        
        df['price_change_15m'] = df['close'].pct_change(1).shift(-1)
        df['price_change_1h'] = df['close'].pct_change(4).shift(-4)
        df['price_change_4h'] = df['close'].pct_change(16).shift(-16)
        
        df['volatility_1h'] = df['close'].rolling(4).std()
        base_threshold = 0.003
        df['dynamic_threshold'] = base_threshold * (1 + df['volatility_1h'].fillna(0))
        
        conditions = [
            (df['price_change_15m'] > df['dynamic_threshold']) & 
            (df['price_change_1h'] > df['dynamic_threshold']) & 
            (df['price_change_4h'] > 0),
            
            (df['price_change_1h'] > df['dynamic_threshold']) & 
            (df['price_change_4h'] > -df['dynamic_threshold']),
            
            (df['price_change_15m'] < -df['dynamic_threshold']) & 
            (df['price_change_1h'] < -df['dynamic_threshold']) & 
            (df['price_change_4h'] < 0),
        ]
        
        choices = [2, 1, 0]
        df['target_final'] = np.select(conditions, choices, default=1)
        
        temp_cols = ['price_change_15m', 'price_change_1h', 'price_change_4h', 'volatility_1h', 'dynamic_threshold']
        df = df.drop(columns=temp_cols)
        
        return df
    
    def split_data_time_series(self, X, y):
        """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å —É—á–µ—Ç–æ–º –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤"""
        train_size = int(len(X) * (1 - CONFIG['TEST_SIZE'] - CONFIG['VAL_SIZE']))
        val_size = int(len(X) * CONFIG['VAL_SIZE'])
        
        X_train = X.iloc[:train_size]
        y_train = y.iloc[:train_size]
        X_val = X.iloc[train_size:train_size + val_size]
        y_val = y.iloc[train_size:train_size + val_size]
        X_test = X.iloc[train_size + val_size:]
        y_test = y.iloc[train_size + val_size:]
        
        logger.info(f"–†–∞–∑–º–µ—Ä—ã –≤—ã–±–æ—Ä–æ–∫: train={len(X_train)}, val={len(X_val)}, test={len(X_test)}")
        return X_train, X_val, X_test, y_train, y_val, y_test
    
    def feature_selection(self, X_train, y_train, X_val, X_test):
        """–û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        logger.info("–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –æ—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        n_features = min(CONFIG['TOP_FEATURES'], X_train.shape[1] // 2)
        
        selector1 = SelectKBest(f_classif, k=n_features)
        selector2 = SelectKBest(mutual_info_classif, k=n_features)
        
        selector1.fit(X_train, y_train)
        selector2.fit(X_train, y_train)
        
        selected_features = set()
        selected_features.update(X_train.columns[selector1.get_support()])
        selected_features.update(X_train.columns[selector2.get_support()])
        
        selected_features = list(selected_features)
        logger.info(f"–û—Ç–æ–±—Ä–∞–Ω–æ {len(selected_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        
        return (X_train[selected_features], 
                X_val[selected_features], 
                X_test[selected_features],
                selected_features)
    
    def train_individual_models(self, X_train, y_train, X_val, y_val):
        """–û–±—É—á–µ–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –∞–Ω—Å–∞–º–±–ª—è"""
        logger.info("–û–±—É—á–µ–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π...")
        
        # LightGBM –º–æ–¥–µ–ª—å
        lgb_model = lgb.LGBMClassifier(
            objective='multiclass',
            num_class=len(np.unique(y_train)),
            metric='multi_logloss',
            boosting_type='gbdt',
            num_leaves=50,
            learning_rate=0.1,
            feature_fraction=0.8,
            bagging_fraction=0.8,
            bagging_freq=5,
            min_child_samples=20,
            reg_alpha=0.1,
            reg_lambda=1.0,
            n_estimators=500,
            verbose=-1,
            random_state=CONFIG['RANDOM_STATE']
        )
        
        # XGBoost –º–æ–¥–µ–ª—å –ë–ï–ó early_stopping_rounds –≤ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–µ
        xgb_model = xgb.XGBClassifier(
            n_estimators=500,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=0.1,
            reg_lambda=1.0,
            random_state=CONFIG['RANDOM_STATE']
        )
        
        # –û–±—É—á–µ–Ω–∏–µ LightGBM
        logger.info("–û–±—É—á–µ–Ω–∏–µ LightGBM...")
        lgb_model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            callbacks=[lgb.early_stopping(stopping_rounds=50)]
        )
        
        # –û–±—É—á–µ–Ω–∏–µ XGBoost —Å early_stopping –≤ fit()
        logger.info("–û–±—É—á–µ–Ω–∏–µ XGBoost...")
        
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å —Å early_stopping –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        xgb_temp = xgb.XGBClassifier(
            n_estimators=1000,  # –ë–æ–ª—å—à–µ –¥–ª—è early stopping
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=0.1,
            reg_lambda=1.0,
            random_state=CONFIG['RANDOM_STATE']
        )
        
        # –û–±—É—á–∞–µ–º —Å early stopping
        xgb_temp.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            early_stopping_rounds=50,
            verbose=False
        )
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é XGBoost –º–æ–¥–µ–ª—å —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∏—Ç–µ—Ä–∞—Ü–∏–π
        xgb_model = xgb.XGBClassifier(
            n_estimators=xgb_temp.best_iteration + 1,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=0.1,
            reg_lambda=1.0,
            random_state=CONFIG['RANDOM_STATE']
        )
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ XGBoost
        xgb_model.fit(X_train, y_train)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        self.models = {
            'lgb': lgb_model,
            'xgb': xgb_model
        }
        
        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        lgb_score = accuracy_score(y_val, lgb_model.predict(X_val))
        xgb_score = accuracy_score(y_val, xgb_model.predict(X_val))
        
        logger.info(f"LightGBM validation accuracy: {lgb_score:.4f}")
        logger.info(f"XGBoost validation accuracy: {xgb_score:.4f}")
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if lgb_score >= xgb_score:
            logger.info("LightGBM –≤—ã–±—Ä–∞–Ω–∞ –∫–∞–∫ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å")
            return lgb_model
        else:
            logger.info("XGBoost –≤—ã–±—Ä–∞–Ω–∞ –∫–∞–∫ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å")
            return xgb_model
    
    def evaluate_models(self, X_test, y_test):
        """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π"""
        logger.info("–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π...")
        results = {}
        
        for name, model in self.models.items():
            y_pred = model.predict(X_test)
            y_pred_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None
            
            metrics = {
                'accuracy': accuracy_score(y_test, y_pred),
                'precision': precision_score(y_test, y_pred, average='weighted'),
                'recall': recall_score(y_test, y_pred, average='weighted'),
                'f1': f1_score(y_test, y_pred, average='weighted')
            }
            
            if y_pred_proba is not None:
                max_proba = np.max(y_pred_proba, axis=1)
                high_confidence_mask = max_proba > 0.7
                
                if np.sum(high_confidence_mask) > 0:
                    metrics['high_confidence_accuracy'] = accuracy_score(
                        y_test[high_confidence_mask], 
                        y_pred[high_confidence_mask]
                    )
                    metrics['high_confidence_samples'] = np.sum(high_confidence_mask)
                else:
                    metrics['high_confidence_accuracy'] = 0.0
                    metrics['high_confidence_samples'] = 0
            
            results[name] = metrics
            
            logger.info(f"\nüìä {name.upper()} Results:")
            for metric, value in metrics.items():
                if isinstance(value, float):
                    logger.info(f"  {metric.replace('_', ' ').title()}: {value:.4f}")
                else:
                    logger.info(f"  {metric.replace('_', ' ').title()}: {value}")
        
        return results
    
    def save_models(self, X_test, y_test, selected_features):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        for name, model in self.models.items():
            model_path = f"{CONFIG['MODEL_DIR']}/final_{name}_model_{timestamp}.pkl"
            
            model_package = {
                'model': model,
                'scaler': self.scaler,
                'selected_features': selected_features,
                'config': CONFIG,
                'timestamp': timestamp,
                'model_type': name
            }
            
            joblib.dump(model_package, model_path)
            logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å: {model_path}")
        
        results = self.evaluate_models(X_test, y_test)
        
        results_path = f"{CONFIG['REPORTS_DIR']}/final_results_{timestamp}.json"
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_path}")
        return results

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    start_time = datetime.now()
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –§–ò–ù–ê–õ–¨–ù–û–ô —Å–∏—Å—Ç–µ–º—ã –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π")
    logger.info(f"–í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞: {start_time}")
    
    try:
        trainer = FinalModelTrainer()
        
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X, y = trainer.load_and_prepare_data(
            symbols=['BTCUSDT', 'ETHUSDT'], 
            timeframes=[15, 30]
        )
        
        # 2. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        X_train, X_val, X_test, y_train, y_val, y_test = trainer.split_data_time_series(X, y)
        
        # 3. –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        X_train, X_val, X_test, selected_features = trainer.feature_selection(
            X_train, y_train, X_val, X_test
        )
        
        # 4. –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
        logger.info("–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)
        X_test_scaled = scaler.transform(X_test)
        
        trainer.scaler = scaler
        
        X_train = pd.DataFrame(X_train_scaled, columns=selected_features)
        X_val = pd.DataFrame(X_val_scaled, columns=selected_features)
        X_test = pd.DataFrame(X_test_scaled, columns=selected_features)
        
        # 5. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        best_model = trainer.train_individual_models(X_train, y_train, X_val, y_val)
        
        # 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results = trainer.save_models(X_test, y_test, selected_features)
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
        end_time = datetime.now()
        duration = end_time - start_time
        
        logger.info("\n" + "="*70)
        logger.info("üéâ –§–ò–ù–ê–õ–¨–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
        logger.info(f"‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {duration}")
        logger.info(f"üèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {max(results.items(), key=lambda x: x[1]['accuracy'])[0]}")
        logger.info(f"üéØ –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {max([r['accuracy'] for r in results.values()]):.4f}")
        logger.info("="*70)
        
        print("\nüìã –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢:")
        print(f"üìä –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {len(X)} —Å—Ç—Ä–æ–∫")
        print(f"üîß –ü—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {len(selected_features)}")
        print(f"‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {duration}")
        print("\nüèÜ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ú–û–î–ï–õ–ï–ô:")
        for name, metrics in results.items():
            print(f"  {name.upper()}: Accuracy = {metrics['accuracy']:.4f}, F1 = {metrics['f1']:.4f}")
        
        print(f"\nüíæ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {CONFIG['MODEL_DIR']}/")
        print(f"üìÑ –û—Ç—á–µ—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {CONFIG['REPORTS_DIR']}/")
        print("\n‚úÖ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—é!")
        print("\n–î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python test_trained_model.py")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è: {str(e)}", exc_info=True)

if __name__ == "__main__":
    main()